model:
  #model: Qwen/Qwen2.5-0.5B
  model: llm-jp/llm-jp-3-1.8b
  #tokenizer: Qwen/Qwen2.5-0.5B
  tokenizer: llm-jp/llm-jp-3-1.8b
  use_cache: False
  max_length: 1024                    #512


train: # huggingfaceのTrainingArgumentsで利用
  output_dir: ./output
  evaluation_strategy: steps
  logging_strategy: steps
  save_strategy: steps
  learning_rate: 1e-4
  num_train_epochs: 1
  per_device_train_batch_size: 8      #8
  per_device_eval_batch_size: 4       #4
  gradient_accumulation_steps: 8      #8
  gradient_checkpointing: True
  weight_decay: 0.01                  # 適当
  warmup_ratio: 0.1                   # 適当
  optim: adamw_torch                  # 適当
  fp16: True
  bf16: False
  dataloader_num_workers: 4          #4 
  eval_steps: 500                     #500
  save_steps: 500                     #500
  logging_steps: 50                   #50 
  run_name: test                      # wandbのプロジェクト名
  save_total_limit: 2
  save_on_each_node: False
  neftune_noise_alpha: 5              # NEFTTune　適当
  deepspeed: ./configs/deepspeed/ds_config_zero_llmjp.json
  report_to: wandb
  
seed: 42

dataset:
  path: ./data/test_data.json  # jsonファイルへのパス
  subset: chunked                     # 既存の設定を維持
  split: train