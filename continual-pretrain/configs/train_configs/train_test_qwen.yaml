model:
  #model: Qwen/Qwen2.5-0.5B
  model: Qwen/Qwen2.5-1.5B
  #model: llm-jp/llm-jp-3-1.8b
  #tokenizer: Qwen/Qwen2.5-0.5B
  tokenizer: Qwen/Qwen2.5-1.5B
  #tokenizer: llm-jp/llm-jp-3-1.8b
  use_cache: False
  max_length: 256


train: # huggingfaceのTrainingArgumentsで利用
  output_dir: ../outputs
  evaluation_strategy: steps
  logging_strategy: steps
  save_strategy: steps
  learning_rate: 1e-5                             #1e-6
  num_train_epochs: 1
  per_device_train_batch_size: 4                  #8
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 2                  #2
  gradient_checkpointing: True
  weight_decay: 0.01 
  warmup_ratio: 0.1 
  optim: adamw_torch 
  fp16: True
  bf16: False
  dataloader_num_workers: 1
  eval_steps: 200
  save_steps: 200
  logging_steps: 50
  run_name: test                                # wandbのプロジェクト名
  save_total_limit: 1
  save_on_each_node: False
  neftune_noise_alpha: 5
  deepspeed: ./configs/deepspeed/ds_config_zero_qwen.json
  report_to: wandb
  
seed: 42

dataset:
  path: ./data/train-00000-of-00034_1.parquet   # parquetファイルへのパス
  subset: chunked                               # 既存の設定を維持
  split: train